{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would you describe TensorFlow in a short sentence? What are its\n",
    "main features? Can you name other popular deep learning libraries?\n",
    ">TensorFlow is an open-source library for numerical computation, particularly well suited and fine-tuned for large-scale Machine Learning. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis and optimization capabilities (with a portable graph format that allows you to train a TensorFlow model in one environment and run it in another), an optimization API based on reverse-mode autodiff, and several powerful APIs such as tf.keras, tf.data, tf.image, tf.signal, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer.\n",
    "\n",
    "2. Is TensorFlow a drop-in replacement for NumPy? What are the main\n",
    "differences between the two?\n",
    "> Although TensorFlow offers most of the functionalities provided by NumPy, it is not a drop-in replacement, for a few reasons. First, the names of the functions are not always the same (for example, tf.reduce_sum() versus np.sum()). Second, some functions do not behave in exactly the same way (for example, tf.transpose() creates a transposed copy of a tensor, while NumPy's T attribute creates a transposed view, without actually copying any data). Lastly, NumPy arrays are mutable, while TensorFlow tensors are not (but you can use a tf.Variable if you need a mutable object).\n",
    "\n",
    "3. Do you get the same result with tf.range(10) and tf.constant(np.\n",
    "arange(10))?\n",
    "> Both tf.range(10) and tf.constant(np.arange(10)) return a one-dimensional tensor containing the integers 0 to 9. However, the former uses 32-bit integers while the latter uses 64-bit integers. Indeed, TensorFlow defaults to 32 bits, while NumPy defaults to 64 bits.\n",
    "\n",
    "4. Can you name six other data structures available in TensorFlow,\n",
    "beyond regular tensors?\n",
    "> Beyond regular tensors, TensorFlow offers several other data structures, including sparse tensors, tensor arrays, ragged tensors, queues, string tensors, and sets. The last two are actually represented as regular tensors, but TensorFlow provides special functions to manipulate them (in tf.strings and tf.sets).\n",
    "\n",
    "5. You can define a custom loss function by writing a function or by\n",
    "subclassing the tf.keras.losses.Loss class. When would you use\n",
    "each option?\n",
    "> When you want to define a custom loss function, in general you can just implement it as a regular Python function. However, if your custom loss function must support some hyperparameters (or any other state), then you should subclass the keras.losses.Loss class and implement the \\__init__() and call() methods. If you want the loss function's hyperparameters to be saved along with the model, then you must also implement the get_config() method.\n",
    "\n",
    "6. Similarly, you can define a custom metric in a function or as a subclass\n",
    "of tf.keras.metrics.Metric. When would you use each option?\n",
    "> Much like custom loss functions, most metrics can be defined as regular Python functions. But if you want your custom metric to support some hyperparameters (or any other state), then you should subclass the keras.metrics.Metric class. Moreover, if computing the metric over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch (e.g., as for the precision and recall metrics), then you should subclass the keras.metrics.Metric class and implement the \\__init__(), update_state(), and result() methods to keep track of a running metric during each epoch. You should also implement the reset_state() method unless all it needs to do is reset all variables to 0.0. If you want the state to be saved along with the model, then you should implement the get_config() method as well.\n",
    "\n",
    "7. When should you create a custom layer versus a custom model?\n",
    "> You should distinguish the internal components of your model (i.e., layers or reusable blocks of layers) from the model itself (i.e., the object you will train). The former should subclass the keras.layers.Layer class, while the latter should subclass the keras.models.Model class.\n",
    "\n",
    "8. What are some use cases that require writing your own custom training\n",
    "loop?\n",
    "> Writing your own custom training loop is fairly advanced, so you should only do it if you really need to. Keras provides several tools to customize training without having to write a custom training loop: callbacks, custom regularizers, custom constraints, custom losses, and so on. You should use these instead of writing a custom training loop whenever possible: writing a custom training loop is more error-prone, and it will be harder to reuse the custom code you write. However, in some cases writing a custom training loop is necessary⁠—for example, if you want to use different optimizers for different parts of your neural network, like in the Wide & Deep paper. A custom training loop can also be useful when debugging, or when trying to understand exactly how training works.\n",
    "\n",
    "9. Can custom Keras components contain arbitrary Python code, or must\n",
    "they be convertible to TF functions?\n",
    "> Custom Keras components should be convertible to TF Functions, which means they should stick to TF operations as much as possible and respect all the rules listed in Chapter 12 (in the TF Function Rules section). If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a tf.py_function() operation (but this will reduce performance and limit your model's portability) or set dynamic=True when creating the custom layer or model (or set run_eagerly=True when calling the model's compile() method).\n",
    "\n",
    "10. What are the main rules to respect if you want a function to be\n",
    "convertible to a TF function?\n",
    "> RULES:\n",
    "\n",
    "* If you call any external library, including NumPy or even the standard\n",
    "    library, this call will run only during tracing; it will not be part of the\n",
    "    graph. Indeed, a TensorFlow graph can only include TensorFlow\n",
    "    constructs (tensors, operations, variables, datasets, and so on). So,\n",
    "    make sure you use tf.reduce_sum() instead of np.sum(),\n",
    "    tf.sort() instead of the built-in sorted() function, and so on\n",
    "    (unless you really want the code to run only during tracing). This has a\n",
    "    few additional implications:\n",
    "    *    If you define a TF function f(x) that just returns\n",
    "        np.random.rand(), a random number will only be generated\n",
    "        when the function is traced, so f(tf.constant(2.)) and\n",
    "        f(tf.constant(3.)) will return the same random number, but\n",
    "        f(tf.constant([2., 3.])) will return a different one. If you\n",
    "        replace np.random.rand() with tf.random.uniform([]), then\n",
    "        a new random number will be generated upon every call, since\n",
    "        the operation will be part of the graph.\n",
    "    *   If your non-TensorFlow code has side effects (such  as logging\n",
    "    something or updating a Python counter), then you should not\n",
    "    expect those side effects to occur every time you call the TF\n",
    "    function, as they will only occur when the function is traced.\n",
    "\n",
    "    * You can wrap arbitrary Python code in a tf.py_function()\n",
    "    operation, but doing so will hinder performance, as TensorFlow\n",
    "    will not be able to do any graph optimization on this code. It will\n",
    "    also reduce portability, as the graph will only run on platforms\n",
    "    where Python is available (and where the right libraries are\n",
    "    installed).\n",
    "\n",
    "* You can call other Python functions or TF functions, but they should\n",
    "follow the same rules, as TensorFlow will capture their operations in\n",
    "the computation graph. Note that these other functions do not need to\n",
    "be decorated with @tf.function.\n",
    "\n",
    "* If the function creates a TensorFlow variable (or any other stateful\n",
    "TensorFlow object, such as a dataset or a queue), it must do so upon\n",
    "the very first call, and only then, or else you will get an exception. It is\n",
    "usually preferable to create variables outside of the TF function (e.g.,\n",
    "in the build() method of a custom layer). If you want to assign a new\n",
    "value to the variable, make sure you call its assign() method instead\n",
    "of using the = operator.\n",
    "\n",
    "* The source code of your Python function should be available to\n",
    "TensorFlow. If the source code is unavailable (for example, if you\n",
    "define your function in the Python shell, which does not give access to\n",
    "the source code, or if you deploy only the compiled *.pyc Python files\n",
    "to production), then the graph generation process will fail or have\n",
    "limited functionality.\n",
    "\n",
    "* TensorFlow will only capture for loops that iterate over a tensor or a\n",
    "tf.data.Dataset (see Chapter 13). Therefore, make sure you use for\n",
    "i in tf.range(x) rather than for i in range(x), or else the loop\n",
    "will not be captured in the graph. Instead, it will run during tracing.\n",
    "(This may be what you want if the for loop is meant to build the\n",
    "graph; for example, to create each layer in a neural network.)\n",
    "\n",
    "* As always, for performance reasons, you should prefer a vectorized\n",
    "implementation whenever you can, rather than using loops.\n",
    "\n",
    "\n",
    "11. When would you need to create a dynamic Keras model? How do you\n",
    "do that? Why not make all your models dynamic?\n",
    "\n",
    "> Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set dynamic=True when creating it. Alternatively, you can set run_eagerly=True when calling the model's compile() method. Making a model dynamic prevents Keras from using any of TensorFlow's graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model's portability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implement a custom layer that performs layer normalization (we will\n",
    "use this type of layer in Chapter 15):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "_Exercise: The `build()` method should define two trainable weights *α* and *β*, both of shape `input_shape[-1:]` and data type `tf.float32`. *α* should be initialized with 1s, and *β* with 0s._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "_Exercise: The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean μ and the variance σ<sup>2</sup> of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return *α*⊗(*X* - μ)/(σ + ε) + *β*, where ⊗ represents itemwise multiplication (`*`) and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self,eps=0.001,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def build(self,batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\",shape=batch_input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",shape=batch_input_shape[-1:],\n",
    "            initializer=\"zeros\")\n",
    "    \n",
    "    def call(self,X):\n",
    "        mean, variance = tf.nn.moments(X,axes=-1,keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"eps\":self.eps}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note that making ε a hyperparameter (eps) was not compulsory. Also note that it's preferable to compute tf.sqrt(variance + self.eps) rather than tf.sqrt(variance) + self.eps. Indeed, the derivative of sqrt(z) is undefined when z=0, so training will bomb whenever the variance vector has at least one component equal to 0. Adding ε within the square root guarantees that this will never happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "_Exercise: Ensure that your custom layer produces the same (or very nearly the same) output as the `tf.keras.layers.LayerNormalization` layer._\n",
    "\n",
    "Let's create one instance of each class, apply them to some data (e.g., the training set), and ensure that the difference is negligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1,1),random_state=42\n",
    ")\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(\n",
    "    X_train_full,y_train_full,random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.9945699253630664e-08>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = LayerNormalization()\n",
    "keras_layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's close enough. To be extra sure, let's make alpha and beta completely random and compare again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.6172254646562578e-08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "random_alpha = np.random.rand(X.shape[-1])\n",
    "random_beta = np.random.rand(X.shape[-1])\n",
    "\n",
    "custom_layer_norm.set_weights([random_alpha,random_beta])\n",
    "keras_layer_norm.set_weights([random_alpha,random_beta])\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.MeanAbsoluteError()(\n",
    "    keras_layer_norm(X),custom_layer_norm(X)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a negligeable difference! Our custom layer works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Train a model using a custom training loop to tackle the Fashion\n",
    "MNIST dataset\n",
    "\n",
    "### a.\n",
    "_Exercise: Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full,y_train_full),(X_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid,X_train = X_train_full[:5000],X_train_full[5000:]\n",
    "y_valid,y_train = y_train_full[:5000],y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "    tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_batch(X,y,batch_size=32):\n",
    "    idx = np.random.randint(len(X),size =batch_size)\n",
    "    return X[idx],y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e129aac5d64d989b9f711624b8c1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0864b34f43e94866b01aed85c8cdd026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34bee8a2f254a7dbaed993cc92de463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e90958132a4ffeab590474bb34163b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a480a9cab2104f98876d72b681586403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d3ea6d40b74fbe9540b12e44400b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 / 5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "with trange(1,n_epochs+1,desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps+1,desc=f\"Epoch {epoch} / {n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch,y_batch = random_batch(X_train,y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch,y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                gradients = tape.gradient(loss,model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch,y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid,y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid,dtype=np.float32),y_pred))\n",
    "            steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "_Exercise: Try using a different optimizer with a different learning rate for the upper layers and the lower layers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "lower_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "    tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "])\n",
    "\n",
    "upper_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    lower_layers,upper_layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_optimizer  = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "upper_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d780f1d75d541e3866c79e90eb93fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5555596f4244e2bcf596a56e26da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f68b4b7d975419bb13ed9a74232fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f56f7ac86294847a7ea8aa8fd312a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeb015222aa4cdab1dde359cd9c11ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99e5b50be8e414bae53644dd048e7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with trange(1, n_epochs+1 ,desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train,y_train)\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch,y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                for layers, optimizer in ((lower_layers,lower_optimizer),\n",
    "                                          (upper_layers,upper_optimizer)):\n",
    "                    gradients = tape.gradient(loss,layers.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients,layers.trainable_variables))\n",
    "                del tape\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch,y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid,y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid,dtype=np.float32),y_pred))\n",
    "            steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
